{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow deep NN\n",
    "\n",
    "### 이 노트북에서는 MNIST 데이터에 TensorFlow library를  적용하는 과정을 살펴봅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorflow 적용해 필요한 hyperparameter 값을 미리 설정한다.\n",
    "\n",
    "Learning_Rate = 1e-4\n",
    "Training_iterations = 1000\n",
    "Dropout = 0.5\n",
    "Batch_Size = 50\n",
    "Validation_size = 2000\n",
    "Image_to_display = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n",
      "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
      "0      1       0       0       0       0       0       0       0       0   \n",
      "1      0       0       0       0       0       0       0       0       0   \n",
      "2      1       0       0       0       0       0       0       0       0   \n",
      "3      4       0       0       0       0       0       0       0       0   \n",
      "4      0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
      "0       0    ...            0         0         0         0         0   \n",
      "1       0    ...            0         0         0         0         0   \n",
      "2       0    ...            0         0         0         0         0   \n",
      "3       0    ...            0         0         0         0         0   \n",
      "4       0    ...            0         0         0         0         0   \n",
      "\n",
      "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
      "0         0         0         0         0         0  \n",
      "1         0         0         0         0         0  \n",
      "2         0         0         0         0         0  \n",
      "3         0         0         0         0         0  \n",
      "4         0         0         0         0         0  \n",
      "\n",
      "[5 rows x 785 columns]\n"
     ]
    }
   ],
   "source": [
    "# train용 data를 설정해준다.\n",
    "data = pd.read_csv('../Digit Recognizer/train.csv')\n",
    "\n",
    "print (data.shape)\n",
    "print (data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "총 42,000개의 행들과 785개의 열로 이루어진 데이터이다. 각 행의 target값은 label colum으로 각 열의 첫 행에 위치한다.\n",
    "각 행의 값들의 하나의 손글씨 사진에 대한 pixel digit값을 나타낸다. \n",
    "총 785개의 열중에서 첫번째 열인 label colum을 제외하면 총 784(28*28)px 형태 임을 알수있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = data.iloc[:,1:].values\n",
    "images = images.astype(np.float)\n",
    "\n",
    "images = np.multiply(images, 1.0 / 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_size => 784\n",
      "image_width => 28\n",
      "image_height => 28\n"
     ]
    }
   ],
   "source": [
    "image_size = images.shape[1]\n",
    "print ('image_size => {0}'.format(image_size))\n",
    "\n",
    "# 여기에서는 모든 이미지가 정사각형 형태이다.\n",
    "image_width = image_height = np.ceil(np.sqrt(image_size)).astype(np.uint8)\n",
    "\n",
    "print('image_width => {0}\\nimage_height => {1}'.format(image_width, image_height))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image의 output을 살펴보기 위해 1차원 형태의 데이터를 2차원 형태로 바꾸어 보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117ed7588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display image\n",
    "def display(img):\n",
    "    # From 784 to 28 * 28 \n",
    "    one_image = img.reshape(image_width, image_height)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(one_image, cmap=cm.binary)\n",
    "\n",
    "display(images[Image_to_display])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아라비아 숫자 8로 보이는 형태가 나타난 것을 알수있다. label 값으로 주어진 0~9 사이의 숫자 중 하나가 나타났다.   \n",
    "이제 이것을 실제 label 값이랑 비교해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_flat(42000)\n",
      "labels_flat[10] => 8\n"
     ]
    }
   ],
   "source": [
    "labels_flat = data[[0]].values.ravel()\n",
    "\n",
    "print('labels_flat({0})'.format(len(labels_flat)))\n",
    "print('labels_flat[{0}] => {1}'.format(Image_to_display, labels_flat[Image_to_display]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_count => 10\n"
     ]
    }
   ],
   "source": [
    "labels_count = np.unique(labels_flat).shape[0]\n",
    "print('labels_count => {0}'.format(labels_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 data의 label값으로 0~9 까지의 숫자 10개가 고루 있다는 것을 알수있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0~9까지의 값은 하나하나의 형태가 scala형태이다. 분류 문제들은 결과값으로 0~9까지의 값이 될 확률을 제시하고 가장 확률이 높은 값을 고르게 하는 경우가 대다수이다. 이 확률 값을 계산하기 위해서는 각각의 label값이 scala로 존재해서는 아니고 10개의 값을 표현 할 수있는 vector의 값으로 존재하여야 한다. 이때 가장 많이 사용되는 것이 'One-Hot-Encoder(vectors)'이다.\n",
    "여기서는 One-Hot-Encoder를 직접 정의하지만 sklearn에서는 이 기능을 자체적으로 제공한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels(42000,10)\n",
      "labels[10] => [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n"
     ]
    }
   ],
   "source": [
    "# convert class labels from scalars to one-hot vectors\n",
    "# 0 => [1 0 0 0 0 0 0 0 0 0]\n",
    "# 1 => [0 1 0 0 0 0 0 0 0 0]\n",
    "# ...\n",
    "# 9 => [0 0 0 0 0 0 0 0 0 1]\n",
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot\n",
    "\n",
    "labels = dense_to_one_hot(labels_flat, labels_count)\n",
    "laebls = labels.astype(np.uint8)\n",
    "\n",
    "print('labels({0[0]},{0[1]})'.format(labels.shape))\n",
    "print('labels[{0}] => {1}'.format(Image_to_display, labels[Image_to_display]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 전체데이터를 training용과 validation용으로 나누어보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images(40000,784)\n",
      "validation_images(2000,784)\n"
     ]
    }
   ],
   "source": [
    "validation_images = images[:Validation_size]\n",
    "validation_labels = labels[:Validation_size]\n",
    "\n",
    "train_images = images[Validation_size:]\n",
    "train_labels = labels[Validation_size:]\n",
    "\n",
    "\n",
    "print('train_images({0[0]},{0[1]})'.format(train_images.shape))\n",
    "print('validation_images({0[0]},{0[1]})'.format(validation_images.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 neural network에 적용하기 위한 단계는 완성되었다.\n",
    "\n",
    "## TensorFlow graph\n",
    "TensorFlow에서는 각 operation의 진행과정을 한눈에 파악할수있는 graph를 제공하고 있다. \n",
    "\n",
    "### Helper functions(Activation functions)\n",
    "NN모델에서는 수많은 weights와 bias가 생성된다. 때문에 초기 weights의 설정이 꽤 중요하다고 볼수있다. 또 여기에서는 활성 함수(각 neuron을 통과할때 사용하는 함수)로 ReLU함수를 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# weight의 초기값 설정\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기에서는 image recognition에 흔히 사용되는 CNN(Convulution Neural Network)를 사용합니다. Stride/step의 크기는 1로 정해준다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 max Pooling값은 2x2 blocks이다. \n",
    "Pooling은 data를 downsampling하기 위해서 사용된다. 2x2 max-pooling은 2차원 pixel 형태의 block에서 max값만 빼내주는 역할을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pooling\n",
    "# [[0,3],\n",
    "#  [4,2]] => 4\n",
    "\n",
    "# [[0,1],\n",
    "#  [1,1]] => 1\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neural network의 좋은 점 어떤 NN이든 layer로 사용될수 있다는 점이다. 이는 어떤 NN의 output은 다른 NN의 input 값이 될 수 있다는 것을 의미한다. 이 NN의 집합이 성능이 좋은  NN을 만들어내고 이것이 Deep Neural Newworks가 된다.\n",
    "\n",
    "여기에서는 우리는 2개의 Hidden Layers을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input & output of NN\n",
    "\n",
    "# images\n",
    "x = tf.placeholder('float', shape=[None, image_size])\n",
    "#labels\n",
    "y_ = tf.placeholder('float', shape=[None, labels_count])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫번째 layer는 max pooling이 적용되는 convolution layer이다. \n",
    "여기서는 5x5x1 크기의 필터를 32개 만들것이다. 이것을 tensor weight의 형태로 나타내면 [5, 5, 1, 32]가 된다. 앞에 두 숫자는 필터의 크기를 의미하고 세번째 숫자 1은 input channel(1은 grayscale image를 의미), 마지막 숫자 32는 output channel을 의미한다.\n",
    "\n",
    "이 layer에 적용시키기 위해 우리 data를 4d tensor 형태로 바꿔준다. 첫번째 숫자는 image의 숫자 두번째와 세번째는 이미지의 가로, 세로 크기 마지막은 색 channel의 숫자를 넣어준다. \n",
    "\n",
    "convolution 이후에 pooling을 적용하여 output의 크기를 28x28에서 14x14로 줄여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first convolutional layer\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "# (40000,784) => (40000,28,28,1)\n",
    "image = tf.reshape(x, [-1, image_width, image_height, 1])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(image, W_conv1) + b_conv1)\n",
    "#print (h_conv1.get_shape()) # => (40000, 28, 28, 32)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "#print (h_pool1.get_shape()) # => (40000, 14, 14, 32)\n",
    "\n",
    "layer1 = tf.reshape(h_conv1, (-1, image_height, image_width, 4 ,8))\n",
    "\n",
    "# reorder so the channels are in the first dimension, x and y follow.\n",
    "layer1 = tf.transpose(layer1, (0, 3, 1, 4,2))\n",
    "layer1 = tf.reshape(layer1, (-1, image_height*4, image_width*8)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두번째 layer는 5x5 크기의 필터와 64개의 feature를 가진 형태다. 이것을 tensor shape로 나타내면 [5, 5, 32, 64]가 된다.\n",
    "각 숫자가 의미하는 바는 이전 layer의 규칙과 동일하다. 여기서 세번째 숫자값이 32는 이전 layer의 output 값인 32이다. 여기서도 또한 각 output channel마다 존재하는 bias vector도 이전 layer와 마찬가지로 정의해준다.\n",
    "\n",
    "이전 layer를 거치면서 14 x 14의 크기가 되었고 두번째 layer에서는 이미지의 일반적인 특성을 더 잘 얻어낼 수 있다. 이는 첫번째 layer에서 보다 섬세한 detail을 잡아냈다면 두번째 layer에서는 image의 대표적인 특성값을 얻어 낼 수 있다는 것을 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# second convolutional layer\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "#print (h_conv2.get_shape()) # => (40000, 14,14, 64)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "#print (h_pool2.get_shape()) # => (40000, 7, 7, 64)\n",
    "\n",
    "# Prepare for visualization\n",
    "# display 64 fetures in 4 by 16 grid\n",
    "layer2 = tf.reshape(h_conv2, (-1, 14, 14, 4, 16))\n",
    "\n",
    "# reorder so the channels are in the first dimension, x and y follow.\n",
    "layer2 = tf.transpose(layer2, (0, 3, 1, 4, 2))\n",
    "layer2 = tf.reshape(layer2, (-1, 14*4, 14*16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 이전에 모든 layer와 연결된 fully-connected layer를 더한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# densely connected layer\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "# (40000, 7, 7, 64) => (40000, 3136)\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "#print (h_fc1.get_shape()) # => (40000, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오퍼피팅을 막기 위해서 dropout를 적용해 줍니다.\n",
    "\n",
    "dropout은 모델링에 모든 node를 적용하는 것이 아니라 일부를 제외하고 모델링에 적용시켜 training data 에 모델이 overfitting되는 것을 방지해 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dropout \n",
    "keep_prob = tf.placeholder('float')\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 multi-classfication 문제에서 유용하게 사용되는 softmax 함수 layer를 더해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# readout layer for deep net\n",
    "W_fc2 = weight_variable([1024, labels_count])\n",
    "b_fc2 = bias_variable([labels_count])\n",
    "\n",
    "y = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "#print (y.get_shape()) # => (40000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "network의 성능을 평가하기 위해서 log-loss function를 사용하고 가중치를 최적화하기 위한 방법으로는 ADAM optimizer를 사용한다.\n",
    "\n",
    "ADAM optimizer는 gradient based optimization algorithm이고, steepest gradient보다 더 민감하게 반응하며 큰 규모와 데이터와 파라미터가 많은 경우에 유용하게 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cost function\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "\n",
    "# optimisation function\n",
    "train_step = tf.train.AdamOptimizer(Learning_Rate).minimize(cross_entropy)\n",
    "\n",
    "# evaluation\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_, 1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과값을 숫자 형태로 도출하기 위해서는 One-Hot-Encoder로 되어있는 값중에서 최고 확률을 가진 값을 나타나게 해주는 과정이 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prediction function\n",
    "#[0.1, 0.9, 0.2, 0.1, 0.1 0.3, 0.5, 0.1, 0.2, 0.3] => 1\n",
    "predict = tf.argmax(y,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 neural network 구조에 대한 정의가 모두 완료되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, validate and predict\n",
    "\n",
    "### Helper function\n",
    "가장 이상적으로는 model training 모든 데이터를 사용하는 것이 좋지만 이 방법은 시간과 비용이 많이 소요된다. 대신에 여기서 우리는 random하게 뽑은 data의 'batches'를 사용할 것이다.\n",
    "이 방법은 stochastic training이라고도 불리며 모든 데이터를 사용하는 방법과 거의 비슷한 결과값을 주면서도 보다 빠르고 저렴하게 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs_completed = 0\n",
    "index_in_eopch = 0\n",
    "num_examples = train_images.shape[0]\n",
    "\n",
    "# serve data by batches\n",
    "def next_batch(batch_size):\n",
    "    \n",
    "    global train_images\n",
    "    global train_labels\n",
    "    global index_in_eopch\n",
    "    global epochs_completed\n",
    "    \n",
    "    start = index_in_eopch\n",
    "    index_in_eopch += batch_size\n",
    "    \n",
    "    # when all trainig data have been already used, it is reorder randomly \n",
    "    if index_in_eopch > num_examples:\n",
    "        # finished epoch\n",
    "        epochs_completed += 1\n",
    "        # shuffle the data\n",
    "        perm = np.arange(num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        train_images = train_images[perm]\n",
    "        train_labels = train_labels[perm]\n",
    "        # start next epoch\n",
    "        start = 0\n",
    "        index_in_poch = batch_size\n",
    "        assert batch_size <= num_examples\n",
    "    end = index_in_eopch\n",
    "    return train_images[start:end], train_labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 이제 TensorFlow session을 시작해보자.\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_accuracy / validation_accuracy => 0.14 / 0.16 for step 0\n",
      "training_accuracy / validation_accuracy => 0.22 / 0.16 for step 1\n",
      "training_accuracy / validation_accuracy => 0.16 / 0.16 for step 2\n",
      "training_accuracy / validation_accuracy => 0.20 / 0.10 for step 3\n",
      "training_accuracy / validation_accuracy => 0.12 / 0.14 for step 4\n",
      "training_accuracy / validation_accuracy => 0.18 / 0.14 for step 5\n",
      "training_accuracy / validation_accuracy => 0.14 / 0.14 for step 6\n",
      "training_accuracy / validation_accuracy => 0.20 / 0.18 for step 7\n",
      "training_accuracy / validation_accuracy => 0.16 / 0.20 for step 8\n",
      "training_accuracy / validation_accuracy => 0.22 / 0.22 for step 9\n",
      "training_accuracy / validation_accuracy => 0.38 / 0.24 for step 10\n",
      "training_accuracy / validation_accuracy => 0.34 / 0.52 for step 20\n",
      "training_accuracy / validation_accuracy => 0.64 / 0.64 for step 30\n",
      "training_accuracy / validation_accuracy => 0.80 / 0.72 for step 40\n",
      "training_accuracy / validation_accuracy => 0.74 / 0.72 for step 50\n",
      "training_accuracy / validation_accuracy => 0.74 / 0.84 for step 60\n",
      "training_accuracy / validation_accuracy => 0.88 / 0.86 for step 70\n",
      "training_accuracy / validation_accuracy => 0.78 / 0.88 for step 80\n",
      "training_accuracy / validation_accuracy => 0.84 / 0.84 for step 90\n",
      "training_accuracy / validation_accuracy => 0.90 / 0.88 for step 100\n",
      "training_accuracy / validation_accuracy => 0.96 / 0.90 for step 200\n",
      "training_accuracy / validation_accuracy => 0.94 / 0.90 for step 300\n",
      "training_accuracy / validation_accuracy => 0.90 / 0.96 for step 400\n",
      "training_accuracy / validation_accuracy => 1.00 / 0.90 for step 500\n",
      "training_accuracy / validation_accuracy => 0.96 / 0.92 for step 600\n",
      "training_accuracy / validation_accuracy => 0.88 / 0.90 for step 700\n"
     ]
    }
   ],
   "source": [
    "# visualisation variables\n",
    "train_accuracies = []\n",
    "validation_accuracies = []\n",
    "x_range = []\n",
    "\n",
    "display_step = 1\n",
    "\n",
    "for i in range(Training_iterations):\n",
    "    # get new batch\n",
    "    batch_xs, batch_ys = next_batch(Batch_Size)\n",
    "    \n",
    "    # check progress on every 1st, 2nd, ...,100th,.... step\n",
    "    if i%display_step == 0 or (i+1) == Training_iterations:\n",
    "        \n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_xs, \n",
    "                                                  y_: batch_ys, \n",
    "                                                  keep_prob: 1.0})       \n",
    "        if(Validation_size):\n",
    "            validation_accuracy = accuracy.eval(feed_dict={ x: validation_images[0:Batch_Size], \n",
    "                                                            y_: validation_labels[0:Batch_Size], \n",
    "                                                            keep_prob: 1.0})                                  \n",
    "            print('training_accuracy / validation_accuracy => %.2f / %.2f for step %d'%(train_accuracy, validation_accuracy, i))\n",
    "            \n",
    "            validation_accuracies.append(validation_accuracy)\n",
    "            \n",
    "        else:\n",
    "             print('training_accuracy => %.4f for step %d'%(train_accuracy, i))\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        x_range.append(i)\n",
    "        \n",
    "        # increase display_step\n",
    "        if i%(display_step*10) == 0 and i:\n",
    "            display_step *= 10\n",
    "    # train on batch\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, keep_prob: Dropout})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 과정이 끝나면 트레이닝에 사용되지 않은 데이터의 정확도를 확인해보는게 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check final accuracy on validation set\n",
    "if (Validation_size):\n",
    "    validation_accuracy = accuracy.eval(feed_dict={x:validation_images})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
